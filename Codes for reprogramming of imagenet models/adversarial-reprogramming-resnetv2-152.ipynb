{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import *\nimport numpy as np \nimport time \nimport scipy\nfrom keras import Sequential\nfrom keras import Model\n\nnum_classes = 1000\ninput_shape = (28, 28, 3)\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# x_train = np.expand_dims(x_train, -1)\n# x_test = np.expand_dims(x_test, -1)\nX_train = np.stack([x_train, x_train, x_train], axis=-1)\nX_test = np.stack([x_test, x_test, x_test], axis=-1)\nY_train = keras.utils.to_categorical(y_train,1000)\nY_test = keras.utils.to_categorical(y_test,1000)\n\ninput_mask = np.pad(np.zeros([1, 28, 28, 3]),\n                            [[0,0], [98, 98], [98, 98], [0,0]],\n                            'constant', constant_values = 1)\ninput_mask.shape\n\n\nimport keras.backend as K\n\nfrom keras import initializers,regularizers\nfrom keras.optimizers import *\nfrom keras.callbacks import *\n\nM = np.ones((224,224,3)).astype('float32')\nM[98:126,98:126,:] = 0\n\nclass AdvLayer(Layer):\n    def __init__(self, W_regularizer=0.05, **kwargs):\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.l2(W_regularizer)\n        super(AdvLayer, self).__init__(**kwargs)\n    def build(self, input_shape):\n        assert len(input_shape) == 4\n        self.W = self.add_weight(name='kernel', \n                                      shape=(224,224,3),\n                                      initializer=self.init, regularizer=self.W_regularizer,\n                                      trainable=True)\n        super(AdvLayer, self).build(input_shape)\n    def call(self, x):\n        prog = K.tanh(self.W*M)\n        out = x + prog\n        return out\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0],input_shape[1],input_shape[2],input_shape[3])\n\nbasemodel = tf.keras.applications.ResNet152V2(weights=\"imagenet\")\nx = Input(shape=input_shape)\nx_aug = ZeroPadding2D(padding=((98,98),(98,98)))(x)\nout = AdvLayer()(x_aug)\nprobs = basemodel(out)\nmodel = Model(inputs=x,outputs=probs)\nmodel.layers[-1].trainable = False\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:35:47.273127Z","iopub.execute_input":"2021-11-23T06:35:47.273705Z","iopub.status.idle":"2021-11-23T06:36:05.087015Z","shell.execute_reply.started":"2021-11-23T06:35:47.273613Z","shell.execute_reply":"2021-11-23T06:36:05.086214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\"advweightsresnetv2.h5\",save_freq='epoch', save_best_only = True, monitor='loss',\n                            save_weights_only=True, mode='min', verbose=0)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.3, patience=2, min_lr=0.000001, mode='auto')\nstopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, mode= 'min')\nhistory = model.fit(X_train,Y_train,epochs = 100,batch_size=64, validation_data = (X_test,Y_test), verbose=1, callbacks=[checkpoint,reduce_lr,stopping])","metadata":{"execution":{"iopub.status.busy":"2021-11-23T06:36:05.088762Z","iopub.execute_input":"2021-11-23T06:36:05.089025Z","iopub.status.idle":"2021-11-23T14:24:44.383442Z","shell.execute_reply.started":"2021-11-23T06:36:05.088989Z","shell.execute_reply":"2021-11-23T14:24:44.381501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T14:24:44.384174Z","iopub.status.idle":"2021-11-23T14:24:44.38447Z","shell.execute_reply.started":"2021-11-23T14:24:44.384316Z","shell.execute_reply":"2021-11-23T14:24:44.384336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:11:17.246602Z","iopub.execute_input":"2021-11-22T06:11:17.247068Z","iopub.status.idle":"2021-11-22T06:11:17.26519Z","shell.execute_reply.started":"2021-11-22T06:11:17.247031Z","shell.execute_reply":"2021-11-22T06:11:17.264324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}